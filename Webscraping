#Import relevant libraries

import csv
import requests
from bs4 import BeautifulSoup
import os
import pandas as pd
import nltk
from nltk.tokenize import RegexpTokenizer, sent_tokenize

from Dictionaries import stopwords, positive, negative


#Variable Definition
def tokenizer(text):
    text = text.lower()
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(text)
    filtered_words = list(filter(lambda token: token not in stopwords, tokens))
    return filtered_words
    
def positive_score(text):
    Positive_Words = 0
    token = tokenizer(text)
    for word in token:
        if word in positive:
            Positive_Words  += 1
    
    sum_of_positive = Positive_Words
    return sum_of_positive
   
def negative_score(text):
    Negative_Words=0
    Token = tokenizer(text)
    for word in Token:
        if word in negative:
            Negative_Words -=1
    sum_of_negative = Negative_Words 
    sum_of_negative = sum_of_negative * -1
    return sum_of_negative
    
def polarity_score(positiveScore, negativeScore):
    polarity_score = (positiveScore - negativeScore) / ((positiveScore + negativeScore) + 0.000001)
    return polarity_score
    
def subjectivity_score(positiveScore, negativeScore, text):
    filtered_words = len(text)
    subjective_score = (positiveScore + negativeScore)/ ((filtered_words) + 0.000001)
    return subjective_score
   
def average_sentence_length(text):
    sentences = sent_tokenize(text)
    tokens = tokenizer(text)
    WordCount = len(tokens)
    totalSentences = len(sentences)
    avg_sentence = 0
    if totalSentences != 0:
        avg_sentence = WordCount / totalSentences
    avg_sentence_length = avg_sentence
    return round(avg_sentence_length)
    
def percentage_complex_word(text):
    tokens = tokenizer(text)
    complexWord = 0
    complex_word_percentage = 0
    
    for word in tokens:
        vowels=0
        if word.endswith(('es','ed')):
            pass
        else:
            for w in word:
                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):
                    vowels += 1
            if(vowels > 2):
                complexWord += 1
    if len(tokens) != 0:
        complex_word_percentage = complexWord/len(tokens)
    return complex_word_percentage
    
def fog_index(avgSentenceLength, percentageComplexWord):
    fogIndex = 0.4 * (avgSentenceLength + percentageComplexWord)
    return fogIndex
    
def avg_words_per_sentence(text):
    sentences = sent_tokenize(text)
    tokens = tokenizer(text)
    WordCount = len(tokens)
    totalSentences = len(sentences)
    avg_sentence = 0
    if totalSentences != 0:
        words_per_sentence = WordCount/totalSentences
    words_per_sentence_avg = words_per_sentence
    
    return round(words_per_sentence_avg)

def complex_word_count(text):
    tokens = tokenizer(text)
    complexWord = 0
    
    for word in tokens:
        vowels=0
        if word.endswith(('es','ed')):
            pass
        else:
            for w in word:
                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):
                    vowels += 1
            if(vowels > 2):
                complexWord += 1
    return complexWord
   
def total_word_count(text):
    tokens = tokenizer(text)
    return len(tokens)
    
def syllables_per_word(text):
    tokens = tokenizer(text)
    syllables = 0
    
    for word in tokens:
        if word.endswith(('es','ed')):
            pass
        else:
            for w in word:
                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):
                    syllables += 1
    syllable_per_word = syllables/len(tokens)
    return syllable_per_word
    
    
def syllables_per_word(text):
    tokens = tokenizer(text)
    syllables = 0
    
    for word in tokens:
        if word.endswith(('es','ed')):
            pass
        else:
            for w in word:
                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):
                    syllables += 1
    syllable_per_word = syllables/len(tokens)
    return syllable_per_word
    
def avg_word_length(text):
    tokens = tokenizer(text)
    word_count = len(tokens)
    word_avg_length = len(text)/word_count
    return word_avg_length
    
a = 'URL'
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36"}
#header workaround for security error

with open(r'C:/Users/HP/Desktop/Output Data.csv', 'w', newline='') as file:#creating our output data file
    writer = csv.writer(file)
    writer.writerow(["URL_ID", "URL", "POSITIVE SCORE", "NEGATIVE SCORE", "POLARITY SCORE", "SUBJECTIVITY SCORE", 
                     "AVG SENTENCE LENGTH", "PERCENTAGE OF COMPLEX WORDS", "FOG INDEX", "AVG NUMBER OF WORDS PER SENTENCE", 
                     "COMPLEX WORD COUNT", "WORD COUNT", "SYLLABLE PER WORD", "PERSONAL PRONOUNS", "AVG WORD LENGTH" ])

read_file = pd.read_excel (r'C:/Users/HP/Downloads/Input.xlsx')
read_file.to_csv (r'C:/Users/HP/Downloads/Input.csv', index = None, header=True)
with open('C:/Users/HP/Downloads/Input.csv', encoding='utf-8') as csvfile:#open csv
    reader = csv.reader(csvfile, delimiter = ',') #checks csv with delimiter and reads accurately
    for row in reader: #for all the rows in the csv
        if any([a in row]):#if any of the rows have URL
            pass #ignore them for now
        else:
            url = row[1] #gets all links in the url column one by one
            response = requests.get(url, headers = headers)
            html = response.content
            soup = BeautifulSoup(html, 'html.parser') #parse the html file from the link
            souptitle = soup.find("h1", class_ = 'entry-title').get_text().encode("utf-8") #get title by using tags on the html page, this is only used bc the structure of all the sites are the same
            souptext = soup.find("div", class_ = 'td-post-content').get_text().encode("utf-8") #get body, same explanation here#
            soupresult = souptitle + souptext #combine the gotten texts for easier coding
            cleanedsoupresult = BeautifulSoup(soupresult, "lxml").get_text(strip = True) #remove extraneous characters
            pScore = positive_score(cleanedsoupresult) #calculate positive score for each cleaned text
            nScore = negative_score(cleanedsoupresult) #calculate negative score for each cleaned text
            #nltk.download('punkt') #only uncomment this line when your nltk packages are incomplete
            averageSentenceLength = average_sentence_length(cleanedsoupresult) #calculation for each text
            percentageComplexWord = percentage_complex_word(cleanedsoupresult) #calculation for each text
            urlfilename = f'{row[0]}.txt' #this gives url_id.txt
            savetopath = os.path.join('C:/Users/HP/Desktop/Blackcoffer' , urlfilename) #sending each txt file to a folder
            with open(savetopath, "w", encoding = "utf-8") as file:
                file.write(str(cleanedsoupresult))#write cleaned text in each txt file
            with open(r'C:/Users/HP/Desktop/Output Data.csv', 'a') as file:
                writer = csv.writer(file)     
                writer.writerow([row[0], row[1], pScore, nScore, polarity_score(pScore, nScore), 
                                 subjectivity_score(pScore, nScore, cleanedsoupresult), averageSentenceLength, 
                                 percentageComplexWord, fog_index(averageSentenceLength, percentageComplexWord), 
                                 avg_words_per_sentence(cleanedsoupresult), complex_word_count(cleanedsoupresult), 
                                 total_word_count(cleanedsoupresult), syllables_per_word(cleanedsoupresult), 
                                 personal_pronouns(cleanedsoupresult), avg_word_length(cleanedsoupresult)]) #calculate variables
                
